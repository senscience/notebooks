{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demographic and Breed Data of Registered Dogs in Zurich (2014-2024) Exploration with `mlcroissant`\n",
    "This notebook provides a template for loading and exploring a dataset using the `mlcroissant` library.\n",
    "\n",
    "### Dataset Source\n",
    "The dataset source is provided via a Croissant schema URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure `mlcroissant` library is installed\n",
    "!pip install mlcroissant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "Load metadata and records from the dataset using `mlcroissant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlcroissant as mlc\n",
    "import pandas as pd\n",
    "\n",
    "# Define the dataset Croissant schema URL\n",
    "croissant_url = 'https://sen.science/doi/10.71728/senscience.834f-x4hv/fair2.json'\n",
    "\n",
    "# Load the dataset metadata\n",
    "dataset = mlc.Dataset(croissant_url)\n",
    "print(f\"Dataset loaded. Metadata @id: {dataset.metadata.id}\")\n",
    "print(f\"Title: {dataset.metadata.name}\")\n",
    "print(f\"Description: {dataset.metadata.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "Review available record sets, fields, and their IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all record sets and their fields using their @id, name, and description\n",
    "if hasattr(dataset, 'record_sets'):\n",
    "    print(\"Available Record Sets:\")\n",
    "    for rs in dataset.record_sets:\n",
    "        print(f\"- Record Set @id: {rs.id}\")\n",
    "        print(f\"  Name: {getattr(rs, 'name', None)}\")\n",
    "        print(f\"  Description: {getattr(rs, 'description', None)}\")\n",
    "        if hasattr(rs, 'fields') and rs.fields:\n",
    "            print(\"  Fields:\")\n",
    "            for field in rs.fields:\n",
    "                print(f\"    - Field @id: {field.id} | Name: {getattr(field, 'name', None)} | Data type: {getattr(field, 'data_type', None)}\")\n",
    "        print(\"\")\n",
    "else:\n",
    "    print(\"No record sets found in the dataset schema.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Extraction\n",
    "Load data from a specific record set into a DataFrame for analysis. Use the record set and field `@id`s from the overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all record set @ids\n",
    "record_set_ids = []\n",
    "if hasattr(dataset, 'record_sets'):\n",
    "    record_set_ids = [rs.id for rs in dataset.record_sets]\n",
    "else:\n",
    "    print(\"Dataset has no record sets.\")\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for record_set_id in record_set_ids:\n",
    "    try:\n",
    "        records = list(dataset.records(record_set=record_set_id))\n",
    "        if records:\n",
    "            df = pd.DataFrame(records)\n",
    "            dataframes[record_set_id] = df\n",
    "            print(f\"Loaded DataFrame for Record Set @id: {record_set_id}\")\n",
    "            print(f\"Columns: {df.columns.tolist()}\")\n",
    "            print(df.head(2))\n",
    "        else:\n",
    "            print(f\"Record set @id {record_set_id} contains no records.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load records for record set @id {record_set_id}: {e}\")\n",
    "\n",
    "# For demonstration, choose the first non-empty record set\n",
    "non_empty_record_set_id = None\n",
    "for k, v in dataframes.items():\n",
    "    if not v.empty:\n",
    "        non_empty_record_set_id = k\n",
    "        break\n",
    "\n",
    "if non_empty_record_set_id:\n",
    "    print(f\"Example DataFrame columns for record set @id {non_empty_record_set_id}:\")\n",
    "    print(dataframes[non_empty_record_set_id].columns.tolist())\n",
    "    display(dataframes[non_empty_record_set_id].head())\n",
    "else:\n",
    "    print(\"No non-empty DataFrames found in record sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "Apply common data processing steps, such as filtering records based on specific criteria, normalizing numeric fields, and categorizing data. This section should include operations like removing outliers, transforming data distributions, or grouping data by key attributes to prepare it for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EDA on the main record set ---\n",
    "if not non_empty_record_set_id:\n",
    "    print(\"No data available for EDA.\")\n",
    "else:\n",
    "    df = dataframes[non_empty_record_set_id]\n",
    "    print(f\"Analyzing DataFrame for Record Set @id: {non_empty_record_set_id}\")\n",
    "    \n",
    "    # Identify numeric fields by data type (e.g., 'birthyear', 'age', or similar columns)\n",
    "    numeric_col_candidates = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    if not numeric_col_candidates:\n",
    "        print(\"No numeric fields found for analysis.\")\n",
    "    else:\n",
    "        numeric_field = numeric_col_candidates[0]\n",
    "        print(f\"Numeric field selected for filtering and normalization: {numeric_field}\")\n",
    "        \n",
    "        # For example, select a threshold. If field is 'birthyear', we can use after 2018, else value>threshold\n",
    "        threshold = df[numeric_field].mean() if df[numeric_field].dtype!=int else 2018\n",
    "        filtered_df = df[df[numeric_field] > threshold]\n",
    "        print(f\"Filtered records with {numeric_field} > {threshold}:\")\n",
    "        print(filtered_df.head())\n",
    "        \n",
    "        # Normalized field\n",
    "        filtered_df = filtered_df.copy()\n",
    "        filtered_df[f\"{numeric_field}_normalized\"] = (filtered_df[numeric_field] - filtered_df[numeric_field].mean()) / filtered_df[numeric_field].std()\n",
    "        print(f\"Normalized {numeric_field} for filtered records:\")\n",
    "        print(filtered_df[[numeric_field, f\"{numeric_field}_normalized\"]].head())\n",
    "        \n",
    "        # Select a group field (for dog data, could be 'breed', 'gender', etc.), fallback to any object dtype\n",
    "        potential_group_fields = [c for c in df.columns if df[c].dtype == 'object' and c != numeric_field]\n",
    "        if potential_group_fields:\n",
    "            group_field = potential_group_fields[0]\n",
    "            print(f\"Grouping by field: {group_field}\")\n",
    "            grouped_df = filtered_df.groupby(group_field)[numeric_field].mean().reset_index().sort_values(numeric_field, ascending=False)\n",
    "            print(f\"Grouped data by {group_field} and calculated mean of {numeric_field}:\")\n",
    "            print(grouped_df.head())\n",
    "        else:\n",
    "            print(\"No suitable object/categorical column for grouping found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "Visualize data distributions or relationships between fields in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "if not non_empty_record_set_id:\n",
    "    print(\"No data available for visualization.\")\n",
    "else:\n",
    "    if not numeric_col_candidates:\n",
    "        print(\"No numeric field present for plotting.\")\n",
    "    else:\n",
    "        # Histogram of numeric field\n",
    "        plt.figure(figsize=(8,5))\n",
    "        sns.histplot(df[numeric_field].dropna(), bins=30, kde=True)\n",
    "        plt.title(f\"Distribution of {numeric_field}\")\n",
    "        plt.xlabel(numeric_field)\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "        \n",
    "        # If grouped_df exists from EDA, show barplot\n",
    "        if 'grouped_df' in locals():\n",
    "            top_n = grouped_df.head(10)\n",
    "            plt.figure(figsize=(10,5))\n",
    "            sns.barplot(x=top_n[group_field], y=top_n[numeric_field])\n",
    "            plt.title(f\"Top 10 Groups by Mean {numeric_field}\")\n",
    "            plt.xlabel(group_field)\n",
    "            plt.ylabel(f\"Mean {numeric_field}\")\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "Summarize key findings and observations from the dataset exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this notebook, we demonstrated how to load a Croissant-structured dataset using `mlcroissant`, inspect its metadata, explore record sets and their fields via their `@id`, extract data into pandas DataFrames, and perform initial exploratory analysis and visualization.\n",
    "- You can extend this notebook by further exploring additional record sets, performing deeper statistical analyses, or building machine learning models using the cleaned dataset.\n",
    "- Always refer to record set, field, and column `@id`s when manipulating and documenting entities for reproducibility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}