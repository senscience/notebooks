{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydrological and Environmental Measurements in Bay of Biscay and Adjacent Waters Exploration with `mlcroissant`\n",
    "This notebook demonstrates how to load, explore, and process the FAIR² dataset using the `mlcroissant` library. \n",
    "\n",
    "### Dataset Source\n",
    "The dataset uses a Croissant schema accessible at <https://sen.science/doi/10.71728/senscience.h3h6-tjan/fair2.json>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mlcroissant. If running in an environment where it's not installed, uncomment the line below:\n",
    "!pip install mlcroissant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "Load metadata and records from the dataset using `mlcroissant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlcroissant as mlc\n",
    "import pandas as pd\n",
    "\n",
    "# The Croissant schema URL for FAIR²\n",
    "croissant_url = \"https://sen.science/doi/10.71728/senscience.h3h6-tjan/fair2.json\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = mlc.Dataset(croissant_url)\n",
    "metadata = dataset.metadata\n",
    "\n",
    "print(f\"{metadata.name}: {metadata.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "Review available record sets, fields, and their `@id`s.\n",
    "\n",
    "We'll list all available record sets, each field's `@id`, and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all record sets and their fields, referencing by @id\n",
    "print(\"Available record sets (@id):\")\n",
    "record_sets = []\n",
    "for record_set in dataset.record_sets:\n",
    "    print(f\"\\nRecord Set: {record_set['@id']}\")\n",
    "    record_sets.append(record_set['@id'])\n",
    "    print(\"  Fields:\")\n",
    "    for field in record_set['field']:\n",
    "        # field may be string (just id) or dict (expanded)\n",
    "        if isinstance(field, dict):\n",
    "            fid = field.get('@id', '')\n",
    "            label = field.get('label', '')\n",
    "        else:\n",
    "            fid = field\n",
    "            label = ''\n",
    "        print(f\"    - {fid} {label}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Extraction\n",
    "Load data from all discovered record sets into Pandas DataFrames for further processing.\n",
    "We use `@id` for referencing each record set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all record sets data to DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# List for sample demonstration (replace with actual record_set ids as discovered)\n",
    "for record_set_id in record_sets:\n",
    "    try:\n",
    "        records = list(dataset.records(record_set=record_set_id))\n",
    "        if records:\n",
    "            df = pd.DataFrame(records)\n",
    "            dataframes[record_set_id] = df\n",
    "            print(f\"Loaded {len(df)} records for {record_set_id}\")\n",
    "            print(f\"Columns: {df.columns.tolist()}\")\n",
    "            display(df.head())\n",
    "        else:\n",
    "            print(f\"No records found for {record_set_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {record_set_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "Let's perform some EDA steps for one of the record sets with tabular numeric data.\n",
    "\n",
    "We'll select one record set with available data and pick a numeric field (by its `@id`) to:\n",
    "- Filter records by a threshold,\n",
    "- Normalize the field,\n",
    "- Group by a categorical field, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a record set with loaded data for EDA\n",
    "import numpy as np\n",
    "# You may pick a record_set_id from previous outputs. For demonstration:\n",
    "eda_record_set_id = None\n",
    "for k, v in dataframes.items():\n",
    "    if not v.select_dtypes(include=np.number).empty:\n",
    "        eda_record_set_id = k\n",
    "        break\n",
    "if eda_record_set_id is None:\n",
    "    print(\"No numeric record set found. Adjust and rerun.\")\n",
    "else:\n",
    "    df = dataframes[eda_record_set_id]\n",
    "    numeric_fields = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    if not numeric_fields:\n",
    "        print(\"No numeric columns found in selected record set.\")\n",
    "    else:\n",
    "        # Use the first numeric field as demo\n",
    "        numeric_field = numeric_fields[0]\n",
    "        print(f\"Analyzing field: {numeric_field}\\n\")\n",
    "\n",
    "        # Filter records (example threshold)\n",
    "        threshold = df[numeric_field].mean() if df[numeric_field].mean() > 0 else 10\n",
    "        filtered_df = df[df[numeric_field] > threshold].copy()\n",
    "        print(f\"Filtered records with {numeric_field} > {threshold:.2f}:\")\n",
    "        display(filtered_df.head())\n",
    "\n",
    "        # Normalize column\n",
    "        std = df[numeric_field].std()\n",
    "        # avoid division by 0\n",
    "        if std == 0:\n",
    "            filtered_df[f\"{numeric_field}_normalized\"] = 0\n",
    "        else:\n",
    "            filtered_df[f\"{numeric_field}_normalized\"] = (filtered_df[numeric_field] - df[numeric_field].mean()) / std\n",
    "\n",
    "        print(f\"\\nNormalized {numeric_field} for filtered records:\")\n",
    "        display(filtered_df[[numeric_field, f\"{numeric_field}_normalized\"]].head())\n",
    "\n",
    "        # Attempt to group by a categorical field\n",
    "        cat_fields = df.select_dtypes(exclude=np.number).columns\n",
    "        group_field = None\n",
    "        for f in cat_fields:\n",
    "            if filtered_df[f].nunique() > 1 and filtered_df[f].nunique() < 20:\n",
    "                group_field = f\n",
    "                break\n",
    "\n",
    "        if group_field:\n",
    "            grouped_df = filtered_df.groupby(group_field)[numeric_field].mean().reset_index()\n",
    "            print(f\"\\nGrouped mean {numeric_field} by {group_field}:\")\n",
    "            display(grouped_df.head())\n",
    "        else:\n",
    "            print(\"No suitable categorical group field found for grouping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "Let's visualize the distribution of the selected numeric field using a histogram and, if grouped, a bar chart of the group means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "if eda_record_set_id is not None and numeric_fields:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    sns.histplot(df[numeric_field].dropna(), bins=30, kde=True, ax=ax[0], color='skyblue')\n",
    "    ax[0].set_title(f\"Distribution of {numeric_field}\")\n",
    "    ax[0].set_xlabel(numeric_field)\n",
    "\n",
    "    if 'grouped_df' in locals() and group_field is not None:\n",
    "        sns.barplot(data=grouped_df, x=group_field, y=numeric_field, ax=ax[1])\n",
    "        ax[1].set_title(f\"Mean {numeric_field} by {group_field}\")\n",
    "        ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=45)\n",
    "    else:\n",
    "        ax[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "In this notebook, we demonstrated how to use `mlcroissant` to load and analyze a multi-record set FAIR² dataset directly from a Croissant schema URL. \n",
    "\n",
    "- We listed the available record sets and their fields by `@id`.\n",
    "- For a selected record set, we performed numeric filtering, normalization, group statistics, and plotted visualizations.\n",
    "\n",
    "You can adapt these steps to other record sets and fields by referencing their `@id` as shown in the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}