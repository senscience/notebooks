{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydrological and Environmental Measurements in Bay of Biscay and Adjacent Waters Exploration with `mlcroissant`\n",
    "This notebook demonstrates loading and exploring the FAIR² hydrological and environmental dataset using the `mlcroissant` library. The dataset is described using a Croissant schema and contains multiple record sets representing water quality, sediment composition, biotic abundance, and contaminant concentrations across the Bay of Biscay region.\n",
    "\n",
    "### Dataset Source\n",
    "The dataset Croissant schema is accessible at:\n",
    "```\n",
    "https://sen.science/doi/10.71728/senscience.h3h6-tjan/fair2.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure `mlcroissant` library is installed\n",
    "!pip install mlcroissant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "Load metadata and records from the dataset using `mlcroissant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlcroissant as mlc\n",
    "import pandas as pd\n",
    "\n",
    "# Define the Croissant schema URL\n",
    "croissant_url = \"https://sen.science/doi/10.71728/senscience.h3h6-tjan/fair2.json\"\n",
    "\n",
    "# Load the dataset metadata\n",
    "dataset = mlc.Dataset(croissant_url)\n",
    "metadata = dataset.metadata\n",
    "\n",
    "print(f\"\\033[1m{metadata.name}\\033[0m\\n{metadata.description}\\n\")\n",
    "print(f\"License: {metadata.license}\")\n",
    "print(f\"Date Published: {metadata.datePublished}\")\n",
    "print(f\"Collection Timeframe: {metadata.dataCollectionTimeframe}\")\n",
    "print(f\"Data Biases: {metadata.dataBiases if hasattr(metadata, 'dataBiases') else 'N/A'}\")\n",
    "print(f\"Data Use Cases: {metadata.dataUseCases if hasattr(metadata, 'dataUseCases') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "Let's review available record sets and their fields. All references will use the unique `@id` for each record set and field, as defined by the Croissant schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all record sets in the dataset using their `@id`\n",
    "record_sets = list(dataset.record_sets)\n",
    "\n",
    "print(\"Available record sets (by @id):\")\n",
    "for rs in record_sets:\n",
    "    print(f\"- {rs['@id']}   ({rs['name']})\")\n",
    "\n",
    "# Show fields/columns in each record set\n",
    "for rs in record_sets:\n",
    "    print(f\"\\nRecord Set '{rs['name']}' (@id: {rs['@id']}):\")\n",
    "    if 'field' in rs and rs['field']:\n",
    "        for field in rs['field']:\n",
    "            # The field may be a dict or a uri string; handle both\n",
    "            field_obj = field if isinstance(field, dict) else dataset.field_by_id(field)\n",
    "            if field_obj is not None:\n",
    "                print(f\"  - {field_obj['@id']}   ({field_obj.get('name', '')})\")\n",
    "    else:\n",
    "        print(\"  No fields listed in record set.\")\n",
    "\n",
    "# For demonstration, show a few example records from the first record set (if present)\n",
    "if record_sets:\n",
    "    print(f\"\\nExample records from {record_sets[0]['@id']}:\")\n",
    "    for idx, rec in enumerate(dataset.records(record_set=record_sets[0]['@id'])):\n",
    "        if idx >= 3:\n",
    "            break\n",
    "        print(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Extraction\n",
    "Load data from selected record sets into Pandas DataFrames for further analysis.\n",
    "\n",
    "Below, we extract all available record sets using their `@id`. The DataFrames are stored in a dictionary with record set `@id` as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dictionary to hold our DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# List of record set @ids\n",
    "record_set_ids = [rs['@id'] for rs in record_sets]\n",
    "\n",
    "for record_set_id in record_set_ids:\n",
    "    try:\n",
    "        records = list(dataset.records(record_set=record_set_id))\n",
    "        if records:\n",
    "            df = pd.DataFrame(records)\n",
    "            dataframes[record_set_id] = df\n",
    "            print(f\"Loaded {len(df)} records for record set '@id': {record_set_id}\")\n",
    "        else:\n",
    "            print(f\"No records found in record set '@id': {record_set_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load record set {record_set_id}: {e}\")\n",
    "\n",
    "# Display columns of the first DataFrame as an example\n",
    "if dataframes:\n",
    "    first_rs_id = next(iter(dataframes))\n",
    "    print(f\"\\nColumns in DataFrame for record set '@id': {first_rs_id}\")\n",
    "    print(dataframes[first_rs_id].columns.tolist())\n",
    "    display(dataframes[first_rs_id].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "Let's select a record set and numeric field (referenced by `@id` per Croissant) for analysis. We'll filter, normalize, and group data based on these fields.\n",
    "\n",
    "*(Edit the `selected_record_set_id`, `numeric_field_id`, and `group_field_id` variables below to explore other parts of the dataset. You can find these `@id`s from previous outputs.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the record set and fields (all by '@id')\n",
    "# (Replace these values with the @id of your interest, using the overview in previous cells)\n",
    "selected_record_set_id = ''\n",
    "numeric_field_id = ''\n",
    "group_field_id = ''\n",
    "\n",
    "# --- Fallback: Attempt to auto-select if not set ---\n",
    "if not selected_record_set_id:\n",
    "    # Select the first DataFrame with numeric fields\n",
    "    for rs_id, df in dataframes.items():\n",
    "        for col in df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                selected_record_set_id = rs_id\n",
    "                numeric_field_id = col\n",
    "                break\n",
    "        if selected_record_set_id:\n",
    "            break\n",
    "if not group_field_id and selected_record_set_id:\n",
    "    # Pick the first non-numeric field as group_field_id\n",
    "    df = dataframes[selected_record_set_id]\n",
    "    for col in df.columns:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            group_field_id = col\n",
    "            break\n",
    "\n",
    "if not selected_record_set_id or not numeric_field_id:\n",
    "    print(\"Unable to auto-select fields. Please set selected_record_set_id and numeric_field_id to valid @id values from your dataset.\")\n",
    "else:\n",
    "    df = dataframes[selected_record_set_id]\n",
    "    print(f\"Using record set '@id': {selected_record_set_id}\")\n",
    "    print(f\"Numeric field for EDA: '{numeric_field_id}'\")\n",
    "    # Filtering\n",
    "    if pd.api.types.is_numeric_dtype(df[numeric_field_id]):\n",
    "        threshold = df[numeric_field_id].quantile(0.50)  # Median as threshold for filtering\n",
    "        filtered_df = df[df[numeric_field_id] > threshold].copy()\n",
    "        print(f\"Filtered {len(filtered_df)}/{len(df)} records with {numeric_field_id} > {threshold}\")\n",
    "        # Normalization\n",
    "        filtered_df[f\"{numeric_field_id}_normalized\"] = (\n",
    "            (filtered_df[numeric_field_id] - filtered_df[numeric_field_id].mean()) /\n",
    "            (filtered_df[numeric_field_id].std() if filtered_df[numeric_field_id].std() > 1e-9 else 1)\n",
    "        )\n",
    "        display(filtered_df[[numeric_field_id, f\"{numeric_field_id}_normalized\"]].head())\n",
    "    else:\n",
    "        print(f\"Field '{numeric_field_id}' is not numeric, cannot filter or normalize.\")\n",
    "\n",
    "    # Grouping\n",
    "    if group_field_id and group_field_id in filtered_df.columns:\n",
    "        grouped_df = filtered_df.groupby(group_field_id)[numeric_field_id].mean().reset_index()\n",
    "        print(f\"\\nMean of '{numeric_field_id}' grouped by '{group_field_id}' (by @id):\")\n",
    "        display(grouped_df.head())\n",
    "    else:\n",
    "        print(f\"No suitable group field available for grouping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "Let's visualize the distribution of the selected numeric field, and (if applicable) compare it across the grouping field.\n",
    "\n",
    "*(The visualizations reference all fields by their `@id`, in alignment with the Croissant schema.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if selected_record_set_id and numeric_field_id and selected_record_set_id in dataframes:\n",
    "    df = dataframes[selected_record_set_id]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(df[numeric_field_id].dropna(), bins=30, kde=True)\n",
    "    plt.title(f\"Distribution of field '@id': {numeric_field_id}\")\n",
    "    plt.xlabel(numeric_field_id)\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "    # Boxplot per group field if available\n",
    "    if group_field_id and group_field_id in df.columns:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.boxplot(x=group_field_id, y=numeric_field_id, data=df)\n",
    "        plt.title(f\"Boxplot of '{numeric_field_id}' by group '@id': {group_field_id}\")\n",
    "        plt.xlabel(group_field_id)\n",
    "        plt.ylabel(numeric_field_id)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "This notebook demonstrated how to load and explore a FAIR² environmental dataset using the `mlcroissant` library and the Croissant schema.\n",
    "\n",
    "- Each record set, field, and data column was referenced and processed according to its unique `@id` identifier, consistent with best practices for schema-based data.\n",
    "- We loaded all available record sets, previewed records, normalized numeric fields, grouped results, and visualized distributions and relationships between fields.\n",
    "- This workflow facilitates reproducible, schema-driven data analysis for complex, multi-table open datasets.\n",
    "\n",
    "**Next steps:** Try switching `record_set_id`, `numeric_field_id`, and `group_field_id` to other values (see Data Overview) to analyze different data tables and fields from the FAIR² dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}