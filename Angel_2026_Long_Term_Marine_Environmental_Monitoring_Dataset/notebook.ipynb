{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-Term Marine Environmental Monitoring Data from Basque Country Estuaries and Coasts, 1995â€“2014 Exploration with `mlcroissant`\n",
    "This notebook provides a step-by-step guide for loading and exploring the FAIR^2 marine dataset using the `mlcroissant` library.\n",
    "\n",
    "### Dataset Source\n",
    "The dataset source is provided via a Croissant schema URL:\n",
    "\n",
    "`https://sen.science/doi/10.71728/senscience.bx5w-rs5c/fair2.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure `mlcroissant` library is installed\n",
    "!pip install mlcroissant --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "Load metadata and records from the dataset using `mlcroissant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlcroissant as mlc\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define the dataset Croissant schema URL\n",
    "croissant_url = 'https://sen.science/doi/10.71728/senscience.bx5w-rs5c/fair2.json'\n",
    "\n",
    "# Load the dataset metadata\n",
    "dataset = mlc.Dataset(croissant_url)\n",
    "metadata = dataset.metadata.to_json()\n",
    "print(f\"{metadata['name']} (ID: {metadata['@id']})\\n\")\n",
    "print(metadata['description'])\n",
    "print(f\"\\nPublished: {metadata['datePublished']}\")\n",
    "print(f\"Spatial Coverage: {metadata['spatialCoverage']}\")\n",
    "print(f\"Temporal Coverage: {metadata['temporalCoverage']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "Review available record sets, fields, and their `@id`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the record sets from the dataset metadata\n",
    "record_sets = metadata.get('recordSet', [])\n",
    "if not record_sets:\n",
    "    print(\"No record sets found in the dataset metadata.\")\n",
    "else:\n",
    "    print(f\"Found {len(record_sets)} record sets. Listing their @id and fields:\")\n",
    "    for rs in record_sets:\n",
    "        rs_id = rs.get('@id', '')\n",
    "        print(f\"\\nRecord set @id: {rs_id}\")\n",
    "        fields = rs.get('field', [])\n",
    "        if fields:\n",
    "            field_ids = [f.get('@id', '') for f in fields]\n",
    "            print(f\"Fields @id: {field_ids}\")\n",
    "        else:\n",
    "            print(\"No fields found in this record set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Extraction\n",
    "Load data from each record set into a DataFrame for analysis. All references use `@id` values as in the FAIR^2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list of record set @id's\n",
    "record_set_ids = []\n",
    "for rs in record_sets:\n",
    "    if '@id' in rs:\n",
    "        record_set_ids.append(rs['@id'])\n",
    "\n",
    "dataframes = {}\n",
    "for record_set_id in record_set_ids:\n",
    "    try:\n",
    "        records = list(dataset.records(record_set=record_set_id))\n",
    "        df = pd.DataFrame(records)\n",
    "        if not df.empty:\n",
    "            dataframes[record_set_id] = df\n",
    "            print(f\"Loaded DataFrame for record set {record_set_id}: {df.shape[0]} rows, {df.shape[1]} columns.\")\n",
    "        else:\n",
    "            print(f\"Record set {record_set_id} yielded an empty DataFrame.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load records for {record_set_id}: {e}\")\n",
    "\n",
    "# Show columns from the first available DataFrame\n",
    "if dataframes:\n",
    "    first_rs_id = list(dataframes.keys())[0]\n",
    "    print(f\"\\nColumns in record set {first_rs_id}:\")\n",
    "    print(dataframes[first_rs_id].columns.tolist())\n",
    "    dataframes[first_rs_id].head()\n",
    "else:\n",
    "    print(\"No data available from any record set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "Apply common data processing steps, such as filtering records based on specific criteria, normalizing numeric fields, and grouping data. \n",
    "All references to fields and columns use their `@id`. \n",
    "\n",
    "Let's perform example processing on the first DataFrame loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA for the first available record set\n",
    "import numpy as np\n",
    "\n",
    "if dataframes:\n",
    "    df = dataframes[first_rs_id]\n",
    "    # Find a numeric column by heuristics or list\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if numeric_cols:\n",
    "        numeric_field_id = numeric_cols[0]\n",
    "        print(f\"Using numeric field: {numeric_field_id}\")\n",
    "        # Set arbitrary threshold for demonstration\n",
    "        threshold = df[numeric_field_id].mean()\n",
    "        filtered_df = df[df[numeric_field_id] > threshold]\n",
    "        print(f\"Filtered records where {numeric_field_id} > mean ({threshold:.2f}): {filtered_df.shape[0]} rows\")\n",
    "\n",
    "        # Normalization (Z-score)\n",
    "        filtered_df[f\"{numeric_field_id}_normalized\"] = (\n",
    "            filtered_df[numeric_field_id] - filtered_df[numeric_field_id].mean()\n",
    "        ) / filtered_df[numeric_field_id].std()\n",
    "        print(filtered_df[[numeric_field_id, f\"{numeric_field_id}_normalized\"]].head())\n",
    "\n",
    "        # Attempt grouping by a common field\n",
    "        potential_group_fields = [c for c in df.columns if c != numeric_field_id]\n",
    "        if potential_group_fields:\n",
    "            group_field_id = potential_group_fields[0]\n",
    "            print(f\"Grouping by field: {group_field_id}\")\n",
    "            grouped_df = filtered_df.groupby(group_field_id)[numeric_field_id].mean().reset_index()\n",
    "            print(grouped_df.head())\n",
    "        else:\n",
    "            print(\"No grouping fields found.\")\n",
    "    else:\n",
    "        print(\"No numeric fields found for EDA.\")\n",
    "else:\n",
    "    print(\"No data loaded; cannot perform EDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "Visualize data distributions or relationships between fields using the extracted DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if dataframes and numeric_cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[numeric_field_id].dropna(), bins=30, kde=True)\n",
    "    plt.title(f\"Distribution of {numeric_field_id} in record set {first_rs_id}\")\n",
    "    plt.xlabel(numeric_field_id)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter plot between first two numeric fields if available\n",
    "    if len(numeric_cols) > 1:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(x=df[numeric_cols[0]], y=df[numeric_cols[1]])\n",
    "        plt.title(f\"Scatter plot: {numeric_cols[0]} vs {numeric_cols[1]} in {first_rs_id}\")\n",
    "        plt.xlabel(numeric_cols[0])\n",
    "        plt.ylabel(numeric_cols[1])\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Visualization skipped: no numeric data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "This notebook demonstrated how to load, explore, and analyze marine environmental monitoring data from the Basque Country using `mlcroissant`.\n",
    "Key steps:\n",
    "- Load metadata and records with schema references by `@id`\n",
    "- Review record sets and fields using their unique `@id`s\n",
    "- Extract tabular data for processing\n",
    "- Filter, normalize, and group data using column `@id`s\n",
    "- Visualize distributions and relationships\n",
    "\n",
    "You can continue by selecting specific record sets and field `@id`s for advanced domain analyses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}